defaults:
  - _self_

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 1e-5
  num_train_epochs: 2
  optim: "adamw_hf"
  lr_scheduler_type: "cosine"
  max_length: 1024
  gradient_checkpointing: true
  bf16: true
  attn_implementation: "flash_attention_2"

data:
  dataset: "/home/asphodel/code/asphodel_diploma/gpo-reward-model-experiments/data/augmented_dataset.parquet"
  dataset_mode: ""
  debug: false

lora:
  use_lora: true
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.05

evaluation:
  per_device_eval_batch_size: 1
  evaluation_strategy: "steps"
  eval_steps: 100

model:
  base_model: "google/gemma-2b-it"
  weight_ratio: 0.01
  beta: 0.1
  layer_type: "mlp"
  num_layers: 1
  num_neurons: 1024
  reference_free: true
  sft_only: true
  no_logsigmoid_sft: false

logging:
  report_to: "none"
  log_dir: "./reward_models_train"
  wandb_name: "test"
  save_strategy: "epoch"
  save_steps: 1000
